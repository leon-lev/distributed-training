{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates the use of distributed training of a deep neural network. It follows the [this tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html).\n",
    "\n",
    "# Initial Setup\n",
    "\n",
    "We will use the functions which are implemented in `partition.py` and `train.py`.\n",
    "When the file `train.py` is run from the terminal, it uses the configuration parameters, which are listed in `config.yaml`. In our initial setup, we will use the following parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size: 2\n partition_sizes: [0.5, 0.5]\n custom_partition: False\n params:\n  lr: 0.01\n  momentum: 0.5\n  use_batch_norm: False\n  async_op: False"
    }
   ],
   "source": [
    "!cat src/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `train.py` will perform distributed training of a neural network model across 2 parallel processes, using the MNIST dataset. Here we will use the internal functions in `train.py`, such that it will be easier to see which exact parameters we'll be modifying at different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 0, epoch 0:  Train Loss 1.310  Train Acc. 0.558  --|-- Val. Loss 0.645 Val. Acc. 0.799\nRank 1, epoch 0:  Train Loss 1.306  Train Acc. 0.561  --|-- Val. Loss 0.642 Val. Acc. 0.803\nRank 0, epoch 1:  Train Loss 0.548  Train Acc. 0.838  --|-- Val. Loss 0.422 Val. Acc. 0.869\nRank 1, epoch 1:  Train Loss 0.539  Train Acc. 0.835  --|-- Val. Loss 0.441 Val. Acc. 0.866\nRank 0, epoch 2:  Train Loss 0.428  Train Acc. 0.873  --|-- Val. Loss 0.363 Val. Acc. 0.893\nRank 1, epoch 2:  Train Loss 0.418  Train Acc. 0.873  --|-- Val. Loss 0.375 Val. Acc. 0.891\nRank 0, epoch 3:  Train Loss 0.369  Train Acc. 0.892  --|-- Val. Loss 0.306 Val. Acc. 0.912\nRank 1, epoch 3:  Train Loss 0.359  Train Acc. 0.895  --|-- Val. Loss 0.322 Val. Acc. 0.906\nRank 1, epoch 4:  Train Loss 0.312  Train Acc. 0.909  --|-- Val. Loss 0.288 Val. Acc. 0.913\nRank 0, epoch 4:  Train Loss 0.320  Train Acc. 0.906  --|-- Val. Loss 0.265 Val. Acc. 0.921\nRank 0, epoch 5:  Train Loss 0.290  Train Acc. 0.915  --|-- Val. Loss 0.258 Val. Acc. 0.927\nRank 1, epoch 5:  Train Loss 0.285  Train Acc. 0.916  --|-- Val. Loss 0.266 Val. Acc. 0.924\nRank 1, epoch 6:  Train Loss 0.261  Train Acc. 0.924  --|-- Val. Loss 0.246 Val. Acc. 0.924\nRank 0, epoch 6:  Train Loss 0.270  Train Acc. 0.921  --|-- Val. Loss 0.238 Val. Acc. 0.928\nRank 0, epoch 7:  Train Loss 0.259  Train Acc. 0.924  --|-- Val. Loss 0.229 Val. Acc. 0.933\nRank 1, epoch 7:  Train Loss 0.251  Train Acc. 0.927  --|-- Val. Loss 0.229 Val. Acc. 0.933\nRank 1, epoch 8:  Train Loss 0.237  Train Acc. 0.931  --|-- Val. Loss 0.227 Val. Acc. 0.930\nRank 0, epoch 8:  Train Loss 0.242  Train Acc. 0.930  --|-- Val. Loss 0.192 Val. Acc. 0.941\nRank 1, epoch 9:  Train Loss 0.226  Train Acc. 0.934  --|-- Val. Loss 0.213 Val. Acc. 0.934\nRank 0, epoch 9:  Train Loss 0.229  Train Acc. 0.933  --|-- Val. Loss 0.200 Val. Acc. 0.939\n"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "from torch.multiprocessing import Process\n",
    "from train import init_process, run\n",
    "\n",
    "try:\n",
    "    with open(r'src/config.yaml') as file:\n",
    "        config_dict = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "except:\n",
    "    print(f'Could not open configuration file. Aborting.')\n",
    "\n",
    "size = config_dict['size']\n",
    "partition_sizes = config_dict['partition_sizes']\n",
    "custom_partition = config_dict['custom_partition']\n",
    "params = config_dict['params']\n",
    "\n",
    "def distributed_training():\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, \n",
    "                    args=(run, rank, size, partition_sizes,\n",
    "                          custom_partition, params))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a decrease in the loss and an increase in accuracy on both training and validation sets.\n",
    "\n",
    "\n",
    "# Unbalanced Partition\n",
    "\n",
    "Let's see what happens if we modify the partition ratio to 70% : 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 1.689  Train Acc. 0.418  --|-- Val. Loss 0.891 Val. Acc. 0.725\nRank 1, epoch 1:  Train Loss 0.711  Train Acc. 0.779  --|-- Val. Loss 0.602 Val. Acc. 0.821\nRank 0, epoch 0:  Train Loss 1.114  Train Acc. 0.631  --|-- Val. Loss 0.526 Val. Acc. 0.837\nRank 1, epoch 2:  Train Loss 0.544  Train Acc. 0.835  --|-- Val. Loss 0.469 Val. Acc. 0.856\nRank 1, epoch 3:  Train Loss 0.457  Train Acc. 0.862  --|-- Val. Loss 0.418 Val. Acc. 0.879\nRank 0, epoch 1:  Train Loss 0.473  Train Acc. 0.857  --|-- Val. Loss 0.371 Val. Acc. 0.888\nRank 1, epoch 4:  Train Loss 0.404  Train Acc. 0.881  --|-- Val. Loss 0.384 Val. Acc. 0.887\nRank 1, epoch 5:  Train Loss 0.363  Train Acc. 0.894  --|-- Val. Loss 0.348 Val. Acc. 0.897\nRank 0, epoch 2:  Train Loss 0.360  Train Acc. 0.896  --|-- Val. Loss 0.314 Val. Acc. 0.907\nRank 1, epoch 6:  Train Loss 0.339  Train Acc. 0.898  --|-- Val. Loss 0.311 Val. Acc. 0.909\nRank 1, epoch 7:  Train Loss 0.312  Train Acc. 0.908  --|-- Val. Loss 0.285 Val. Acc. 0.917\nRank 1, epoch 8:  Train Loss 0.297  Train Acc. 0.914  --|-- Val. Loss 0.269 Val. Acc. 0.922\nRank 0, epoch 3:  Train Loss 0.304  Train Acc. 0.911  --|-- Val. Loss 0.272 Val. Acc. 0.920\nRank 1, epoch 9:  Train Loss 0.286  Train Acc. 0.916  --|-- Val. Loss 0.273 Val. Acc. 0.922\nProcess Process-3:\nTraceback (most recent call last):\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"src/train.py\", line 131, in init_process\n    fn(rank, size, *args)\n  File \"src/train.py\", line 123, in run\n    train_model(train_set, val_set, **params)\n  File \"src/train.py\", line 79, in train_model\n    average_gradients(model, async_op)\n  File \"src/train.py\", line 49, in average_gradients\n    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM, async_op=async_op)\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 905, in all_reduce\n    work.wait()\nRuntimeError: [/opt/conda/conda-bld/pytorch_1587428207430/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [127.0.1.1]:14258\n"
    }
   ],
   "source": [
    "partition_sizes = [0.7, 0.3]\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the process has failed with an error due to a lack of synchronization between the two processes. This can be solved by setting the *async_op* parameter in the dist.all_reduce function. Let's observed what happens if we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 1.997  Train Acc. 0.291  --|-- Val. Loss 1.266 Val. Acc. 0.582\nRank 1, epoch 1:  Train Loss 0.959  Train Acc. 0.698  --|-- Val. Loss 0.774 Val. Acc. 0.768\nRank 0, epoch 0:  Train Loss 1.394  Train Acc. 0.522  --|-- Val. Loss 0.645 Val. Acc. 0.801\nRank 1, epoch 2:  Train Loss 0.690  Train Acc. 0.787  --|-- Val. Loss 0.586 Val. Acc. 0.814\nRank 1, epoch 3:  Train Loss 0.577  Train Acc. 0.824  --|-- Val. Loss 0.531 Val. Acc. 0.842\nRank 0, epoch 1:  Train Loss 0.574  Train Acc. 0.826  --|-- Val. Loss 0.458 Val. Acc. 0.862\nRank 1, epoch 4:  Train Loss 0.528  Train Acc. 0.839  --|-- Val. Loss 0.502 Val. Acc. 0.847\nRank 1, epoch 5:  Train Loss 0.457  Train Acc. 0.863  --|-- Val. Loss 0.449 Val. Acc. 0.871\nRank 0, epoch 2:  Train Loss 0.437  Train Acc. 0.871  --|-- Val. Loss 0.383 Val. Acc. 0.885\nRank 1, epoch 6:  Train Loss 0.430  Train Acc. 0.871  --|-- Val. Loss 0.457 Val. Acc. 0.859\nRank 1, epoch 7:  Train Loss 0.399  Train Acc. 0.878  --|-- Val. Loss 0.369 Val. Acc. 0.899\nRank 1, epoch 8:  Train Loss 0.370  Train Acc. 0.890  --|-- Val. Loss 0.347 Val. Acc. 0.897\nRank 0, epoch 3:  Train Loss 0.365  Train Acc. 0.892  --|-- Val. Loss 0.323 Val. Acc. 0.905\nRank 1, epoch 9:  Train Loss 0.360  Train Acc. 0.894  --|-- Val. Loss 0.338 Val. Acc. 0.900\nRank 0, epoch 4:  Train Loss 0.322  Train Acc. 0.906  --|-- Val. Loss 0.284 Val. Acc. 0.916\nRank 0, epoch 5:  Train Loss 0.301  Train Acc. 0.910  --|-- Val. Loss 0.276 Val. Acc. 0.918\nRank 0, epoch 6:  Train Loss 0.286  Train Acc. 0.916  --|-- Val. Loss 0.260 Val. Acc. 0.925\nRank 0, epoch 7:  Train Loss 0.266  Train Acc. 0.923  --|-- Val. Loss 0.236 Val. Acc. 0.930\nRank 0, epoch 8:  Train Loss 0.255  Train Acc. 0.925  --|-- Val. Loss 0.235 Val. Acc. 0.929\nRank 0, epoch 9:  Train Loss 0.242  Train Acc. 0.928  --|-- Val. Loss 0.216 Val. Acc. 0.936\n"
    }
   ],
   "source": [
    "params['async_op'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the training process has completed smoothly.\n",
    "\n",
    "# Adding Batch Normalization\n",
    "\n",
    "Next, let's see how the addition of Batch Normalization affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 1.500  Train Acc. 0.511  --|-- Val. Loss 0.913 Val. Acc. 0.723\nRank 1, epoch 1:  Train Loss 0.753  Train Acc. 0.776  --|-- Val. Loss 0.623 Val. Acc. 0.817\nRank 0, epoch 0:  Train Loss 1.004  Train Acc. 0.682  --|-- Val. Loss 0.504 Val. Acc. 0.845\nRank 1, epoch 2:  Train Loss 0.557  Train Acc. 0.833  --|-- Val. Loss 0.482 Val. Acc. 0.861\nRank 1, epoch 3:  Train Loss 0.445  Train Acc. 0.869  --|-- Val. Loss 0.412 Val. Acc. 0.882\nRank 0, epoch 1:  Train Loss 0.444  Train Acc. 0.870  --|-- Val. Loss 0.357 Val. Acc. 0.898\nRank 1, epoch 4:  Train Loss 0.386  Train Acc. 0.884  --|-- Val. Loss 0.373 Val. Acc. 0.889\nRank 1, epoch 5:  Train Loss 0.333  Train Acc. 0.904  --|-- Val. Loss 0.318 Val. Acc. 0.913\nRank 1, epoch 6:  Train Loss 0.313  Train Acc. 0.908  --|-- Val. Loss 0.287 Val. Acc. 0.920\nRank 0, epoch 2:  Train Loss 0.350  Train Acc. 0.899  --|-- Val. Loss 0.280 Val. Acc. 0.916\nRank 1, epoch 7:  Train Loss 0.283  Train Acc. 0.918  --|-- Val. Loss 0.280 Val. Acc. 0.920\nRank 1, epoch 8:  Train Loss 0.272  Train Acc. 0.920  --|-- Val. Loss 0.263 Val. Acc. 0.927\nRank 0, epoch 3:  Train Loss 0.302  Train Acc. 0.913  --|-- Val. Loss 0.256 Val. Acc. 0.924\nRank 1, epoch 9:  Train Loss 0.251  Train Acc. 0.929  --|-- Val. Loss 0.239 Val. Acc. 0.932\nRank 0, epoch 4:  Train Loss 0.260  Train Acc. 0.926  --|-- Val. Loss 0.233 Val. Acc. 0.932\nRank 0, epoch 5:  Train Loss 0.238  Train Acc. 0.932  --|-- Val. Loss 0.212 Val. Acc. 0.938\nRank 0, epoch 6:  Train Loss 0.221  Train Acc. 0.936  --|-- Val. Loss 0.205 Val. Acc. 0.941\nRank 0, epoch 7:  Train Loss 0.212  Train Acc. 0.940  --|-- Val. Loss 0.185 Val. Acc. 0.946\nRank 0, epoch 8:  Train Loss 0.202  Train Acc. 0.942  --|-- Val. Loss 0.174 Val. Acc. 0.951\nRank 0, epoch 9:  Train Loss 0.190  Train Acc. 0.945  --|-- Val. Loss 0.181 Val. Acc. 0.947\n"
    }
   ],
   "source": [
    "params['use_batch_norm'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that slight improvement has been achieved due to the regularizing effect of batch normalization.\n",
    "\n",
    "# Training with Disjoint Subsets of Samples\n",
    "\n",
    "Let's see what happens if we split the samples such that process 1 will only see labels 0-4, and process 2 will only see labels 5-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 0.745  Train Acc. 0.743  --|-- Val. Loss 3.822 Val. Acc. 0.428\nRank 0, epoch 0:  Train Loss 0.581  Train Acc. 0.824  --|-- Val. Loss 4.068 Val. Acc. 0.485\nRank 1, epoch 1:  Train Loss 0.265  Train Acc. 0.920  --|-- Val. Loss 4.191 Val. Acc. 0.450\nRank 0, epoch 1:  Train Loss 0.192  Train Acc. 0.948  --|-- Val. Loss 4.606 Val. Acc. 0.491\nRank 1, epoch 2:  Train Loss 0.188  Train Acc. 0.945  --|-- Val. Loss 4.624 Val. Acc. 0.457\nRank 0, epoch 2:  Train Loss 0.143  Train Acc. 0.960  --|-- Val. Loss 4.629 Val. Acc. 0.498\nRank 1, epoch 3:  Train Loss 0.164  Train Acc. 0.952  --|-- Val. Loss 4.731 Val. Acc. 0.459\nRank 0, epoch 3:  Train Loss 0.117  Train Acc. 0.969  --|-- Val. Loss 4.892 Val. Acc. 0.498\nRank 1, epoch 4:  Train Loss 0.133  Train Acc. 0.961  --|-- Val. Loss 4.882 Val. Acc. 0.462\nRank 0, epoch 4:  Train Loss 0.101  Train Acc. 0.974  --|-- Val. Loss 4.995 Val. Acc. 0.503\nRank 1, epoch 5:  Train Loss 0.143  Train Acc. 0.958  --|-- Val. Loss 5.056 Val. Acc. 0.461\nRank 0, epoch 5:  Train Loss 0.092  Train Acc. 0.976  --|-- Val. Loss 5.217 Val. Acc. 0.500\nRank 1, epoch 6:  Train Loss 0.119  Train Acc. 0.966  --|-- Val. Loss 4.994 Val. Acc. 0.469\nRank 0, epoch 6:  Train Loss 0.083  Train Acc. 0.979  --|-- Val. Loss 5.345 Val. Acc. 0.498\nRank 1, epoch 7:  Train Loss 0.104  Train Acc. 0.971  --|-- Val. Loss 5.155 Val. Acc. 0.472\nRank 0, epoch 7:  Train Loss 0.076  Train Acc. 0.980  --|-- Val. Loss 5.283 Val. Acc. 0.505\nRank 1, epoch 8:  Train Loss 0.093  Train Acc. 0.973  --|-- Val. Loss 5.255 Val. Acc. 0.470\nRank 0, epoch 8:  Train Loss 0.074  Train Acc. 0.979  --|-- Val. Loss 5.444 Val. Acc. 0.503\nRank 1, epoch 9:  Train Loss 0.087  Train Acc. 0.976  --|-- Val. Loss 5.440 Val. Acc. 0.473\nRank 0, epoch 9:  Train Loss 0.071  Train Acc. 0.980  --|-- Val. Loss 5.481 Val. Acc. 0.501\n"
    }
   ],
   "source": [
    "partition_sizes = [0.5, 0.5]\n",
    "custom_partition = True\n",
    "params['async_op'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such setup clearly results in an overfit, since every process is exposed to only half of the labels. Resultantly, on each process, the optimization results in an overfit, which cannot be balanced by simple averaging of the weights, due to the highly non-linear nature of the optimization objective."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitpytorchenvconda608fb9d3a36541f0b05984bd5c41683b",
   "display_name": "Python 3.8.3 64-bit ('pytorch_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}