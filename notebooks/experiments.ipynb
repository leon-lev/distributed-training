{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates the use of distributed training of a deep neural network. It follows the [this tutorial](https://pytorch.org/tutorials/intermediate/dist_tuto.html).\n",
    "\n",
    "# Initial Setup\n",
    "\n",
    "We will use the functions which are implemented in `partition.py` and `train.py`.\n",
    "When the file `train.py` is run from the terminal, it uses the configuration parameters, which are listed in `config.yaml`. In our initial setup, we will use the following parameters: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "size: 2\n partition_sizes: [0.5, 0.5]\n custom_partition: False\n params:\n  lr: 0.01\n  momentum: 0.5\n  use_batch_norm: False\n  async_op: False"
    }
   ],
   "source": [
    "!cat config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `train.py` will perform distributed training of a neural network model across 2 parallel processes, using the MNIST dataset. Here we will use the internal functions in `train.py`, such that it will be easier to see which exact parameters we'll be modifying at different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 0, epoch 0:  Train Loss 1.310  Train Acc. 0.558  --|-- Val. Loss 0.645 Val. Acc. 0.799\nRank 1, epoch 0:  Train Loss 1.306  Train Acc. 0.561  --|-- Val. Loss 0.642 Val. Acc. 0.803\nRank 0, epoch 1:  Train Loss 0.548  Train Acc. 0.838  --|-- Val. Loss 0.422 Val. Acc. 0.869\nRank 1, epoch 1:  Train Loss 0.539  Train Acc. 0.835  --|-- Val. Loss 0.441 Val. Acc. 0.866\nRank 0, epoch 2:  Train Loss 0.428  Train Acc. 0.873  --|-- Val. Loss 0.363 Val. Acc. 0.893\nRank 1, epoch 2:  Train Loss 0.418  Train Acc. 0.873  --|-- Val. Loss 0.375 Val. Acc. 0.891\nRank 1, epoch 3:  Train Loss 0.359  Train Acc. 0.895  --|-- Val. Loss 0.322 Val. Acc. 0.906\nRank 0, epoch 3:  Train Loss 0.369  Train Acc. 0.892  --|-- Val. Loss 0.306 Val. Acc. 0.912\nRank 1, epoch 4:  Train Loss 0.312  Train Acc. 0.909  --|-- Val. Loss 0.288 Val. Acc. 0.913\nRank 0, epoch 4:  Train Loss 0.320  Train Acc. 0.906  --|-- Val. Loss 0.265 Val. Acc. 0.921\nRank 0, epoch 5:  Train Loss 0.290  Train Acc. 0.915  --|-- Val. Loss 0.258 Val. Acc. 0.927\nRank 1, epoch 5:  Train Loss 0.285  Train Acc. 0.916  --|-- Val. Loss 0.266 Val. Acc. 0.924\nRank 0, epoch 6:  Train Loss 0.270  Train Acc. 0.921  --|-- Val. Loss 0.238 Val. Acc. 0.928\nRank 1, epoch 6:  Train Loss 0.261  Train Acc. 0.924  --|-- Val. Loss 0.246 Val. Acc. 0.924\nRank 0, epoch 7:  Train Loss 0.259  Train Acc. 0.924  --|-- Val. Loss 0.229 Val. Acc. 0.933\nRank 1, epoch 7:  Train Loss 0.251  Train Acc. 0.927  --|-- Val. Loss 0.229 Val. Acc. 0.933\nRank 1, epoch 8:  Train Loss 0.237  Train Acc. 0.931  --|-- Val. Loss 0.227 Val. Acc. 0.930\nRank 0, epoch 8:  Train Loss 0.242  Train Acc. 0.930  --|-- Val. Loss 0.192 Val. Acc. 0.941\nRank 0, epoch 9:  Train Loss 0.229  Train Acc. 0.933  --|-- Val. Loss 0.200 Val. Acc. 0.939\nRank 1, epoch 9:  Train Loss 0.226  Train Acc. 0.934  --|-- Val. Loss 0.213 Val. Acc. 0.934\n"
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "from torch.multiprocessing import Process\n",
    "from train import init_process, run\n",
    "\n",
    "try:\n",
    "    with open(r'config.yaml') as file:\n",
    "        config_dict = yaml.load(file, Loader=yaml.SafeLoader)\n",
    "except:\n",
    "    print(f'Could not open configuration file {config_file}. Aborting.')\n",
    "\n",
    "size = config_dict['size']\n",
    "partition_sizes = config_dict['partition_sizes']\n",
    "custom_partition = config_dict['custom_partition']\n",
    "params = config_dict['params']\n",
    "\n",
    "def distributed_training():\n",
    "    processes = []\n",
    "    for rank in range(size):\n",
    "        p = Process(target=init_process, \n",
    "                    args=(run, rank, size, partition_sizes,\n",
    "                          custom_partition, params))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a decrease in the loss and an increase in accuracy on both training and validation sets.\n",
    "\n",
    "\n",
    "# Unbalanced Partition\n",
    "\n",
    "Let's see what happens if we modify the partition ratio to 70% : 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 1.689  Train Acc. 0.418  --|-- Val. Loss 0.891 Val. Acc. 0.725\nRank 1, epoch 1:  Train Loss 0.711  Train Acc. 0.779  --|-- Val. Loss 0.602 Val. Acc. 0.821\nRank 0, epoch 0:  Train Loss 1.114  Train Acc. 0.631  --|-- Val. Loss 0.526 Val. Acc. 0.837\nRank 1, epoch 2:  Train Loss 0.544  Train Acc. 0.835  --|-- Val. Loss 0.469 Val. Acc. 0.856\nRank 1, epoch 3:  Train Loss 0.457  Train Acc. 0.862  --|-- Val. Loss 0.418 Val. Acc. 0.879\nRank 0, epoch 1:  Train Loss 0.473  Train Acc. 0.857  --|-- Val. Loss 0.371 Val. Acc. 0.888\nRank 1, epoch 4:  Train Loss 0.404  Train Acc. 0.881  --|-- Val. Loss 0.384 Val. Acc. 0.887\nRank 1, epoch 5:  Train Loss 0.363  Train Acc. 0.894  --|-- Val. Loss 0.348 Val. Acc. 0.897\nRank 0, epoch 2:  Train Loss 0.360  Train Acc. 0.896  --|-- Val. Loss 0.314 Val. Acc. 0.907\nRank 1, epoch 6:  Train Loss 0.339  Train Acc. 0.898  --|-- Val. Loss 0.311 Val. Acc. 0.909\nRank 1, epoch 7:  Train Loss 0.312  Train Acc. 0.908  --|-- Val. Loss 0.285 Val. Acc. 0.917\nRank 1, epoch 8:  Train Loss 0.297  Train Acc. 0.914  --|-- Val. Loss 0.269 Val. Acc. 0.922\nRank 0, epoch 3:  Train Loss 0.304  Train Acc. 0.911  --|-- Val. Loss 0.272 Val. Acc. 0.920\nRank 1, epoch 9:  Train Loss 0.286  Train Acc. 0.916  --|-- Val. Loss 0.273 Val. Acc. 0.922\nProcess Process-3:\nTraceback (most recent call last):\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n    self.run()\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/multiprocessing/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/leon/source/distributed-training/train.py\", line 131, in init_process\n    fn(rank, size, *args)\n  File \"/home/leon/source/distributed-training/train.py\", line 123, in run\n    train_model(train_set, val_set, **params)\n  File \"/home/leon/source/distributed-training/train.py\", line 79, in train_model\n    average_gradients(model, async_op)\n  File \"/home/leon/source/distributed-training/train.py\", line 49, in average_gradients\n    dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM, async_op=async_op)\n  File \"/home/leon/anaconda3/envs/pytorch_env/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py\", line 905, in all_reduce\n    work.wait()\nRuntimeError: [/opt/conda/conda-bld/pytorch_1587428207430/work/third_party/gloo/gloo/transport/tcp/pair.cc:575] Connection closed by peer [127.0.1.1]:58783\n"
    }
   ],
   "source": [
    "partition_sizes = [0.7, 0.3]\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the process has failed with an error due to a lack of synchronization between the two processes. This can be solved by setting the *async_op* parameter in the dist.all_reduce function. Let's observed what happens if we do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 2.027  Train Acc. 0.280  --|-- Val. Loss 1.302 Val. Acc. 0.571\nRank 1, epoch 1:  Train Loss 0.968  Train Acc. 0.694  --|-- Val. Loss 0.761 Val. Acc. 0.767\nRank 0, epoch 0:  Train Loss 1.364  Train Acc. 0.535  --|-- Val. Loss 0.644 Val. Acc. 0.803\nRank 1, epoch 2:  Train Loss 0.687  Train Acc. 0.788  --|-- Val. Loss 0.581 Val. Acc. 0.821\nRank 1, epoch 3:  Train Loss 0.564  Train Acc. 0.829  --|-- Val. Loss 0.512 Val. Acc. 0.848\nRank 0, epoch 1:  Train Loss 0.578  Train Acc. 0.823  --|-- Val. Loss 0.468 Val. Acc. 0.858\nRank 1, epoch 4:  Train Loss 0.495  Train Acc. 0.849  --|-- Val. Loss 0.473 Val. Acc. 0.853\nRank 1, epoch 5:  Train Loss 0.444  Train Acc. 0.867  --|-- Val. Loss 0.433 Val. Acc. 0.875\nRank 1, epoch 6:  Train Loss 0.414  Train Acc. 0.877  --|-- Val. Loss 0.378 Val. Acc. 0.883\nRank 0, epoch 2:  Train Loss 0.449  Train Acc. 0.867  --|-- Val. Loss 0.395 Val. Acc. 0.882\nRank 1, epoch 7:  Train Loss 0.377  Train Acc. 0.886  --|-- Val. Loss 0.356 Val. Acc. 0.899\nRank 1, epoch 8:  Train Loss 0.359  Train Acc. 0.894  --|-- Val. Loss 0.340 Val. Acc. 0.899\nRank 0, epoch 3:  Train Loss 0.378  Train Acc. 0.889  --|-- Val. Loss 0.333 Val. Acc. 0.903\nRank 1, epoch 9:  Train Loss 0.342  Train Acc. 0.899  --|-- Val. Loss 0.322 Val. Acc. 0.908\nRank 0, epoch 4:  Train Loss 0.333  Train Acc. 0.903  --|-- Val. Loss 0.291 Val. Acc. 0.914\nRank 0, epoch 5:  Train Loss 0.307  Train Acc. 0.909  --|-- Val. Loss 0.284 Val. Acc. 0.914\nRank 0, epoch 6:  Train Loss 0.289  Train Acc. 0.915  --|-- Val. Loss 0.265 Val. Acc. 0.922\nRank 0, epoch 7:  Train Loss 0.270  Train Acc. 0.920  --|-- Val. Loss 0.239 Val. Acc. 0.931\nRank 0, epoch 8:  Train Loss 0.258  Train Acc. 0.924  --|-- Val. Loss 0.236 Val. Acc. 0.932\nRank 0, epoch 9:  Train Loss 0.244  Train Acc. 0.927  --|-- Val. Loss 0.221 Val. Acc. 0.934\n"
    }
   ],
   "source": [
    "params['async_op'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the training process has completed smoothly.\n",
    "\n",
    "# Adding Batch Normalization\n",
    "\n",
    "Next, let's see how the addition of Batch Normalization affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 1.483  Train Acc. 0.515  --|-- Val. Loss 0.903 Val. Acc. 0.729\nRank 1, epoch 1:  Train Loss 0.745  Train Acc. 0.777  --|-- Val. Loss 0.617 Val. Acc. 0.818\nRank 0, epoch 0:  Train Loss 1.002  Train Acc. 0.684  --|-- Val. Loss 0.504 Val. Acc. 0.846\nRank 1, epoch 2:  Train Loss 0.548  Train Acc. 0.836  --|-- Val. Loss 0.465 Val. Acc. 0.867\nRank 1, epoch 3:  Train Loss 0.435  Train Acc. 0.872  --|-- Val. Loss 0.416 Val. Acc. 0.876\nRank 0, epoch 1:  Train Loss 0.430  Train Acc. 0.874  --|-- Val. Loss 0.340 Val. Acc. 0.904\nRank 1, epoch 4:  Train Loss 0.418  Train Acc. 0.875  --|-- Val. Loss 0.391 Val. Acc. 0.884\nRank 1, epoch 5:  Train Loss 0.342  Train Acc. 0.901  --|-- Val. Loss 0.320 Val. Acc. 0.911\nRank 0, epoch 2:  Train Loss 0.330  Train Acc. 0.904  --|-- Val. Loss 0.264 Val. Acc. 0.925\nRank 1, epoch 6:  Train Loss 0.325  Train Acc. 0.905  --|-- Val. Loss 0.307 Val. Acc. 0.913\nRank 1, epoch 7:  Train Loss 0.291  Train Acc. 0.915  --|-- Val. Loss 0.282 Val. Acc. 0.921\nRank 1, epoch 8:  Train Loss 0.278  Train Acc. 0.919  --|-- Val. Loss 0.268 Val. Acc. 0.926\nRank 0, epoch 3:  Train Loss 0.276  Train Acc. 0.921  --|-- Val. Loss 0.236 Val. Acc. 0.934\nRank 1, epoch 9:  Train Loss 0.264  Train Acc. 0.925  --|-- Val. Loss 0.240 Val. Acc. 0.935\nRank 0, epoch 4:  Train Loss 0.247  Train Acc. 0.930  --|-- Val. Loss 0.224 Val. Acc. 0.935\nRank 0, epoch 5:  Train Loss 0.231  Train Acc. 0.934  --|-- Val. Loss 0.207 Val. Acc. 0.941\nRank 0, epoch 6:  Train Loss 0.214  Train Acc. 0.938  --|-- Val. Loss 0.205 Val. Acc. 0.940\nRank 0, epoch 7:  Train Loss 0.207  Train Acc. 0.942  --|-- Val. Loss 0.185 Val. Acc. 0.949\nRank 0, epoch 8:  Train Loss 0.199  Train Acc. 0.942  --|-- Val. Loss 0.173 Val. Acc. 0.952\nRank 0, epoch 9:  Train Loss 0.188  Train Acc. 0.945  --|-- Val. Loss 0.179 Val. Acc. 0.945\n"
    }
   ],
   "source": [
    "params['use_batch_norm'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that slight improvement has been achieved due to the regularizing effect of batch normalization.\n",
    "\n",
    "# Training with Disjoint Subsets of Samples\n",
    "\n",
    "Let's see what happens if we split the samples such that process 1 will only see labels 0-4, and process 2 will only see labels 5-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Rank 1, epoch 0:  Train Loss 0.744  Train Acc. 0.743  --|-- Val. Loss 3.945 Val. Acc. 0.428\nRank 0, epoch 0:  Train Loss 0.573  Train Acc. 0.825  --|-- Val. Loss 3.998 Val. Acc. 0.484\nRank 1, epoch 1:  Train Loss 0.264  Train Acc. 0.919  --|-- Val. Loss 4.316 Val. Acc. 0.449\nRank 0, epoch 1:  Train Loss 0.191  Train Acc. 0.948  --|-- Val. Loss 4.451 Val. Acc. 0.489\nRank 1, epoch 2:  Train Loss 0.191  Train Acc. 0.943  --|-- Val. Loss 4.682 Val. Acc. 0.462\nRank 0, epoch 2:  Train Loss 0.142  Train Acc. 0.961  --|-- Val. Loss 4.475 Val. Acc. 0.497\nRank 1, epoch 3:  Train Loss 0.165  Train Acc. 0.952  --|-- Val. Loss 4.757 Val. Acc. 0.459\nRank 0, epoch 3:  Train Loss 0.114  Train Acc. 0.970  --|-- Val. Loss 4.814 Val. Acc. 0.497\nRank 1, epoch 4:  Train Loss 0.134  Train Acc. 0.960  --|-- Val. Loss 4.926 Val. Acc. 0.461\nRank 0, epoch 4:  Train Loss 0.100  Train Acc. 0.974  --|-- Val. Loss 4.841 Val. Acc. 0.503\nRank 1, epoch 5:  Train Loss 0.137  Train Acc. 0.961  --|-- Val. Loss 5.196 Val. Acc. 0.461\nRank 0, epoch 5:  Train Loss 0.092  Train Acc. 0.976  --|-- Val. Loss 5.073 Val. Acc. 0.499\nRank 1, epoch 6:  Train Loss 0.118  Train Acc. 0.967  --|-- Val. Loss 5.118 Val. Acc. 0.469\nRank 0, epoch 6:  Train Loss 0.082  Train Acc. 0.979  --|-- Val. Loss 5.194 Val. Acc. 0.499\nRank 1, epoch 7:  Train Loss 0.101  Train Acc. 0.972  --|-- Val. Loss 5.231 Val. Acc. 0.472\nRank 0, epoch 7:  Train Loss 0.076  Train Acc. 0.979  --|-- Val. Loss 5.144 Val. Acc. 0.506\nRank 1, epoch 8:  Train Loss 0.095  Train Acc. 0.972  --|-- Val. Loss 5.370 Val. Acc. 0.469\nRank 0, epoch 8:  Train Loss 0.072  Train Acc. 0.980  --|-- Val. Loss 5.384 Val. Acc. 0.503\nRank 1, epoch 9:  Train Loss 0.088  Train Acc. 0.975  --|-- Val. Loss 5.503 Val. Acc. 0.473\nRank 0, epoch 9:  Train Loss 0.066  Train Acc. 0.983  --|-- Val. Loss 5.487 Val. Acc. 0.502\n"
    }
   ],
   "source": [
    "partition_sizes = [0.5, 0.5]\n",
    "custom_partition = True\n",
    "params['async_op'] = True\n",
    "distributed_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such setup clearly results in an overfit, since every process is exposed to only half of the labels. Resultantly, on each process, the optimization results in an overfit, which cannot be balanced by simple averaging of the weights, due to the highly non-linear nature of the optimization objective."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitpytorchenvconda608fb9d3a36541f0b05984bd5c41683b",
   "display_name": "Python 3.8.3 64-bit ('pytorch_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}